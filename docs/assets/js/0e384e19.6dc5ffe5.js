"use strict";(self.webpackChunkmedia_processor_docs=self.webpackChunkmedia_processor_docs||[]).push([[671],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>u});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(a),u=r,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||i;return a?n.createElement(h,o(o({ref:t},c),{},{components:a})):n.createElement(h,o({ref:t},c))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},9881:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const i={sidebar_position:1},o="Vonage ML Transformers",s={unversionedId:"intro",id:"intro",title:"Vonage ML Transformers",description:"Vonage ML transformers is a library that implements machine learning algorithms for the web. This library is based on @vonage/media-processor, MediaPipe and TFLite",source:"@site/docs/intro.md",sourceDirName:".",slug:"/intro",permalink:"/ml-transformers-docs/docs/intro",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"VirtualBackgroundConfig",permalink:"/ml-transformers-docs/docs/api/interfaces/VirtualBackgroundConfig"}},l={},p=[{value:"@vonage/media-processor",id:"vonagemedia-processor",level:3},{value:"MediaPipe",id:"mediapipe",level:3},{value:"Sample applications",id:"sample-applications",level:2},{value:"Background visual effects (out-of-the-box solution)",id:"background-visual-effects-out-of-the-box-solution",level:2},{value:"Implementation details:",id:"implementation-details",level:3},{value:"Configure",id:"configure",level:3},{value:"Blur:",id:"blur",level:4},{value:"Silhouette:",id:"silhouette",level:4},{value:"Virtual (image):",id:"virtual-image",level:4},{value:"Video:",id:"video",level:4},{value:"Create Media Processor",id:"create-media-processor",level:3},{value:"Change configuration",id:"change-configuration",level:3},{value:"Errors, Warnings and Statistics",id:"errors-warnings-and-statistics",level:3},{value:"isSupported",id:"issupported",level:4},{value:"Emitter Registration",id:"emitter-registration",level:4},{value:"Frame Drop warning",id:"frame-drop-warning",level:4},{value:"Statistics",id:"statistics",level:4},{value:"Turn statistics on:",id:"turn-statistics-on",level:5},{value:"Turn statistics off: (by default the statistics are off)",id:"turn-statistics-off-by-default-the-statistics-are-off",level:5},{value:"MediaPipe Helper",id:"mediapipe-helper",level:2},{value:"Configure MediaPipe solution",id:"configure-mediapipe-solution",level:3},{value:"Face Mesh:",id:"face-mesh",level:4},{value:"Face Detection:",id:"face-detection",level:4},{value:"Hands:",id:"hands",level:4},{value:"Holistic:",id:"holistic",level:4},{value:"Objectron:",id:"objectron",level:4},{value:"Pose:",id:"pose",level:4},{value:"Selfie Segmentation:",id:"selfie-segmentation",level:4},{value:"MediaPipe Helper",id:"mediapipe-helper-1",level:3},{value:"Create MediaPipe helper:",id:"create-mediapipe-helper",level:4},{value:"Using MediaPipe helper class:",id:"using-mediapipe-helper-class",level:4},{value:"Create transformer:",id:"create-transformer",level:4},{value:"Use the transformer:",id:"use-the-transformer",level:4},{value:"License",id:"license",level:2}],c={toc:p};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"vonage-ml-transformers"},"Vonage ML Transformers"),(0,r.kt)("p",null,"Vonage ML transformers is a library that implements machine learning algorithms for the web. This library is based on ",(0,r.kt)("em",{parentName:"p"},(0,r.kt)("a",{parentName:"em",href:"https://www.npmjs.com/package/@vonage/media-processor"},"@vonage/media-processor")),", ",(0,r.kt)("em",{parentName:"p"},(0,r.kt)("a",{parentName:"em",href:"https://google.github.io/mediapipe/getting_started/javascript.html"},"MediaPipe"))," and ",(0,r.kt)("em",{parentName:"p"},(0,r.kt)("a",{parentName:"em",href:"https://www.tensorflow.org/lite"},"TFLite"))),(0,r.kt)("h3",{id:"vonagemedia-processor"},"@vonage/media-processor"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Media Processor")," library is Vonage implementation for insertable streams for supported browsers. Documentation can be found ",(0,r.kt)("a",{parentName:"p",href:"https://vonage.github.io/media-processor-docs/"},"here"),"."),(0,r.kt)("h3",{id:"mediapipe"},"MediaPipe"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"MediaPipe")," library is an open source library under MIT license, this library use for video enhancements.\nFor our solution of background blur/replacement we use the ",(0,r.kt)("a",{parentName:"p",href:"https://google.github.io/mediapipe/solutions/selfie_segmentation.html"},"Selfie Segmentation")," solution of MediaPipe.\nThe library adds the support for all MediaPipe JS solutions. This helps developers create cool things with any MediaPipe JS module."),(0,r.kt)("p",null,"For example:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Funny hats"),(0,r.kt)("li",{parentName:"ul"},"Dynamic zoom"),(0,r.kt)("li",{parentName:"ul"},"Eyes glaze"),(0,r.kt)("li",{parentName:"ul"},"Hands detection"),(0,r.kt)("li",{parentName:"ul"},"And much more...")),(0,r.kt)("h2",{id:"sample-applications"},"Sample applications"),(0,r.kt)("p",null,"Sample applications can be found ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Vonage/vonage-media-transformers-samples"},"here"),"."),(0,r.kt)("h2",{id:"background-visual-effects-out-of-the-box-solution"},"Background visual effects (out-of-the-box solution)"),(0,r.kt)("p",null,"This sample uses the Vonage Video web SDK (OpenTok). ",(0,r.kt)("a",{parentName:"p",href:"https://tokbox.com/developer/guides/vonage-media-processor/js/#publisher-setvideomediaprocessorconnector-method"},"OT.Publisher API (setVideoMediaProcessorConnector)")," to use the Vonage Media Processor Library in a Vonage Video (OpenTok) web application."),(0,r.kt)("h3",{id:"implementation-details"},"Implementation details:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Uses the ",(0,r.kt)("a",{parentName:"li",href:"https://www.npmjs.com/package/@mediapipe/selfie_segmentation"},"MediaPipe Selfie Segmentation")," solution."),(0,r.kt)("li",{parentName:"ul"},"The process runs in a web worker."),(0,r.kt)("li",{parentName:"ul"},"MediaPipe solutions are based on WebGL and wasm (SIMD)."),(0,r.kt)("li",{parentName:"ul"},"The solution does not come with MediaPipe binaries bundled. We added static assets under AWS Cloud Front CDN. Here are ",(0,r.kt)("a",{parentName:"li",href:"https://d7uri8nf7uskq.cloudfront.net/tools/list-cloudfront-ips"},"white-listed IPs for cloud front"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"MediaProcessorConfig")," allows you to define ",(0,r.kt)("inlineCode",{parentName:"li"},"mediapipeBaseAssetsUri")," which allows the user to self-host MediaPipe assets. ",(0,r.kt)("strong",{parentName:"li"},"However, we do NOT recommend this"),".")),(0,r.kt)("h3",{id:"configure"},"Configure"),(0,r.kt)("p",null,"Configure post process action."),(0,r.kt)("h4",{id:"blur"},"Blur:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let config: MediaProcessorConfig\nconfig = {\n    mediapipeBaseAssetsUri: 'https://example.com', //This is optional, the library by default provides static assets.\n    transformerType: 'BackgroundBlur',\n    radius: BlurRadius.Low | BlurRadius.High | number //Low=5px High=10px number=(number)px\n}\n")),(0,r.kt)("h4",{id:"silhouette"},"Silhouette:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let config: MediaProcessorConfig\nconfig = {\n    mediapipeBaseAssetsUri: 'https://example.com', // mediapipeBaseAssetsUri is optional Vonage provide static assets for it\n    transformerType: 'SilhouetteBlur',\n    radius: BlurRadius.Low | BlurRadius.High | number //Low=5px High=10px number=(number)px\n}\n")),(0,r.kt)("h4",{id:"virtual-image"},"Virtual (image):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let config: MediaProcessorConfig\nconfig = {\n    mediapipeBaseAssetsUri: 'https://example.com', // mediapipeBaseAssetsUri is optional Vonage provide static assets for it\n    transformerType: 'VirtualBackground',\n    backgroundAssetUri: 'https://some-url-to-image.com'\n}\n")),(0,r.kt)("h4",{id:"video"},"Video:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let config: MediaProcessorConfig\nconfig = {\n    mediapipeBaseAssetsUri: 'https://example.com', // mediapipeBaseAssetsUri is optional Vonage provide static assets for it\n    transformerType: 'VideoBackground',\n    backgroundAssetUri: 'https://some-url-to-video.com'\n}\n")),(0,r.kt)("h3",{id:"create-media-processor"},"Create Media Processor"),(0,r.kt)("p",null,"After configuring which post process is needed, use the helper function to create it ",(0,r.kt)("em",{parentName:"p"},"VonageMediaProcessor")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"const processor = await createVonageMediaProcessor(config);\npublisher.setVideoMediaProcessorConnector(processor.getConnector());\n")),(0,r.kt)("h3",{id:"change-configuration"},"Change configuration"),(0,r.kt)("p",null,"To change the post process config in-flight, you can call this method without involving the publisher ",(0,r.kt)("inlineCode",{parentName:"p"},"setBackgroundOptions")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"await processor.setBackgroundOptions(newConfig);\n")),(0,r.kt)("h3",{id:"errors-warnings-and-statistics"},"Errors, Warnings and Statistics"),(0,r.kt)("h4",{id:"issupported"},"isSupported"),(0,r.kt)("p",null,"Checks if the current browser can run our library."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"try {\n    await  isSupported();\n} catch(e) {\n    console.error(e);\n}\n")),(0,r.kt)("h4",{id:"emitter-registration"},"Emitter Registration"),(0,r.kt)("p",null,"This solution supports ",(0,r.kt)("a",{parentName:"p",href:"https://www.npmjs.com/package/emittery"},"Emittery"),"\nYou can listen event directly on ",(0,r.kt)("inlineCode",{parentName:"p"},"VonageMediaProcessor")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"processor.on('error', ((eventData: ErrorData) => {\n    console.error(eventData);\n}))\nprocessor.on('warn', ((eventData: WarnData) => {\n    console.warn(eventData);\n}))\nprocessor.on('pipelineInfo', ( (eventData: PipelineInfoData) => {\n    console.info(eventData)\n}))\n")),(0,r.kt)("h4",{id:"frame-drop-warning"},"Frame Drop warning"),(0,r.kt)("p",null,"If you like to be notified about frame rate drop use ",(0,r.kt)("inlineCode",{parentName:"p"},"setTrackExpectedRate(number)")," for the expected rate of the process."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"processor.setTrackExpectedRate(30)//or any other value.\n")),(0,r.kt)("h4",{id:"statistics"},"Statistics"),(0,r.kt)("p",null,"The API collect statistics for usage and debugging purposes. However, it is up to the user to activate it."),(0,r.kt)("h5",{id:"turn-statistics-on"},"Turn statistics on:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"const  metadata: VonageMetadata = {\n    appId:  'video SDK app id',\n    sourceType:  'video',\n    proxyUrl: 'https://some-proxy.com' //optional\n};\nsetVonageMetadata(metadata)\n")),(0,r.kt)("h5",{id:"turn-statistics-off-by-default-the-statistics-are-off"},"Turn statistics off: (by default the statistics are off)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"setVonageMetadata(null)\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"That's all you need to do in order to use our out-of-the-box background solution")),(0,r.kt)("h2",{id:"mediapipe-helper"},"MediaPipe Helper"),(0,r.kt)("p",null,"The library provide helper class for all ",(0,r.kt)("a",{parentName:"p",href:"https://google.github.io/mediapipe/getting_started/javascript.html"},"MediaPipe JS solutions"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Face Mesh"),(0,r.kt)("li",{parentName:"ul"},"Face Detection"),(0,r.kt)("li",{parentName:"ul"},"Hands"),(0,r.kt)("li",{parentName:"ul"},"Holistic"),(0,r.kt)("li",{parentName:"ul"},"Objectron"),(0,r.kt)("li",{parentName:"ul"},"Pose"),(0,r.kt)("li",{parentName:"ul"},"Selfie Segmentation")),(0,r.kt)("h3",{id:"configure-mediapipe-solution"},"Configure MediaPipe solution"),(0,r.kt)("p",null,"Each configuration is up to the user."),(0,r.kt)("h4",{id:"face-mesh"},"Face Mesh:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: FaceMeshOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"face-detection"},"Face Detection:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: FaceDetectionOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"hands"},"Hands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: HandsOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"holistic"},"Holistic:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: HolisticOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"objectron"},"Objectron:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: ObjectronOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"pose"},"Pose:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: PoseOptions = {\n...\n}\n")),(0,r.kt)("h4",{id:"selfie-segmentation"},"Selfie Segmentation:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"let  option: SelfieSegmentationOptions = {\n...\n}\n")),(0,r.kt)("h3",{id:"mediapipe-helper-1"},"MediaPipe Helper"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"MediapipeHelper")," - Helper class that initiate and run MediaPipe modules.\n",(0,r.kt)("strong",{parentName:"p"},"This class must be initialized on the application main thread")),(0,r.kt)("h4",{id:"create-mediapipe-helper"},"Create MediaPipe helper:"),(0,r.kt)("p",null,"In this example we will use ",(0,r.kt)("em",{parentName:"p"},"face mash"),", but it is the same for all the other models."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"mediaPipeListener(results: FaceMeshResults): void {\n    //Do something with the results.\n}\nlet mediapipeConfig: MediapipeConfig = {\n    modelType: \"face_mesh\"\n    listener: (results: FaceMeshResults): void => {\n    },\n    options: FaceMeshOptions,\n    assetsUri: 'https://some-url-to-facemash-binaries.com' //Optional - Vonage provides static assets to all MediaPipe modules.\n}\nlet mediapipeHelper: MediapipeHelper = new MediapipeHelper()\nmediapipeHelper.initialize(mediapipeConfig).then( () => {\n}).catch( e => {\n})\n")),(0,r.kt)("h4",{id:"using-mediapipe-helper-class"},"Using MediaPipe helper class:"),(0,r.kt)("p",null,"In this example we will demonstrate how to use the MediaPipe helper with a transformer running on the main application thread.\nHowever, we have two sample apps that run the MediaPipe helper on the main application thread and, concurrently,  the transformer in a Web worker thread."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Auto zoom - Using face detection to create zoom on the main person. ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/Vonage/vonage-media-transformers-samples/tree/main/examples/zoomAndCenterPublisher"},"here"),"."),(0,r.kt)("li",{parentName:"ol"},"Custom MediaPipe: MediaPipe can run both on application main thread and Web worker thread ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/Vonage/vonage-media-transformers-samples/tree/main/examples/mediapipe/customMediaPipe"},"here"),".")),(0,r.kt)("h4",{id:"create-transformer"},"Create transformer:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"class  MedipipeTransformer  implements  Transformer {\n    mediapipeHelper: MediapipeHelper\n    results?: FaceMeshResults\n    constructor(message: string) {\n        this.mediapipeHelper = new MediapipeHelper()\n    }\n    \n    init():Promise<void>{\n        return  new  Promise<void>((resolve, reject) => {\n            let mediapipeConfig: MediapipeConfig = {\n                modelType: \"face_mesh\"\n                listener: (results: FaceMeshResults): void => {\n                    this.results = results\n                },\n                options: FaceMeshOptions,\n                assetsUri: 'https://some-url-to-facemash-binaries.com' //Optional - Vonage provides static assets to all MediaPipe modules.\n            }\n            mediapipeHelper.initialize(mediapipeConfig).then( () => {\n                resolve()\n            }).catch( e => {\n                reject(e)\n            })\n        })\n    }\n    \n    //start function is optional.\n    start(controller:TransformStreamDefaultController) {\n        //In this sample nothing needs to be done.\n    }\n    \n    //transform function is mandatory.\n    transform(frame: VideoFrame, controller: TransformStreamDefaultController) {\n        createImageBitmap(frame).then( image => {\n            let timestamp = frame.timestamp\n            frame.close()\n            this.mediapipeHelper_.send(image).then( () => {\n                if(this.results){\n                    //Do something\n                    controller.enqueue(/*new video frame*/, {timestamp})\n                }\n            }).catch( e => {\n                console.error(e)\n                controller.enqueue(frame)\n            })\n            this.processFrame(image, timestamp, controller)\n        }).catch(e => {\n            console.error(e)\n            controller.enqueue(frame)\n        })\n    }\n    \n    //When using MediaPipe helper close function must be called to avoid memory leaks.\n    flush(controller:TransformStreamDefaultController) {\n        this.mediapipeHelper_.close().then( () => {\n        }).catch( e => {\n            console.error(e)\n        })\n    }\n}\nexport  default  MedipipeTransformer;\n")),(0,r.kt)("h4",{id:"use-the-transformer"},"Use the transformer:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"const mediapipeTransformer: MedipipeTransformer = new MedipipeTransformer()\nmediapipeTransformer.init().then( () => {\n    const mediaProcessor: MediaProcessor = new MediaProcessor()\n    const transformers = [ mediapipeTransformer ]\n    mediaProcessor.setTransformers(transformers)\n    const connector: MediaProcessorConnector = new MediaProcessorConnector(mediaProcessor)\n    ...\n    publisher.setVideoMediaProcessorConnector(connector)\n    ...\n}).catch(e => {\n})\n")),(0,r.kt)("h2",{id:"license"},"License"),(0,r.kt)("p",null,"This project is licensed under the terms of the ",(0,r.kt)("a",{parentName:"p",href:"https://opensource.org/licenses/MIT"},"MIT license")," and is available for free."))}d.isMDXComponent=!0}}]);